to use hdfs commands first you need to start hadoop services using the following command:
$ sbin/start-all.sh

// list all nodes
$ jps

// listing files and directories
$ hdfs dfs -ls dirname[optional]

// copy file from local filesystem to hdfs
$ hdfs dfs -put source destination
$ hdfs dfs -copyFromLocal source destination

// create directory
$ hdfs dfs -mkdir pathToTheDir

// delete the dir
$ hdfs dfs -rmdir dirPath

// delete a file
$ hdfs dfs -rm filePath

$ hdfs dfs -l -R dirname[optional]   // recursively

// see disk usage
$ hdfs dfs -du

// create a empty file
$ hdfs dfs -touchz filePath

// print file contents
$ hdfs dfs -cat filePath

// copy files from hdfs to local system
$ hdfs dfs -get source destination
$ hdfs dfs -copyToLocal source destination

// move file from local system to hdfs
$ hdfs dfs -moveFromLocal source destination

// copy the file within the hdfs
$ hdfs dfs -cp source destination

// move file within the hdfs
$ hdfs dfs -mv source destination

// deletes a file from hdfs recursively
$ hdfs dfs -rmr filePath/dirPath

// get the total size of dir or file
$ hdfs dfs -dus dirName

// stats of file or directory
$ hdfs dfs -stat hdfsFile

// change the replication Factor of a file and directory in hdfs
$ hdfs dfs -setrep -R -w 6 filename

$ dfs classpath [--global | --jar <path> | -h | --help ]

// display computed hadoop env variables
$ hdfs envvars

// get the location , block about the file
$ hdfs fsck filePathandName -files -locations -blocks

// takes the source file and outputs in text format
$ hdfs dfs -text /directoryname/fileName


// count the number of directories, files, 
// and bytes under the paths that match the specified file pattern.
hdfs dfs -count <path>

// make trash empty
hdfs dfs -expunge

// get the help for individual command
$ hdfs dfs -usage <command>
# hdfs dfs -help

// to fetch delegationToken and store it in a file on the local system.
https://blog.cloudera.com/hadoop-delegation-tokens-explained/
fetchdt 


$ hadoop namenode

// format the namenode
$ hadoop namenode -format

$ hadoop datanode

$ hadoop balancer

$ hdfs dfs admin safemode

$ hdfs dfs -getmerge dir1 dir2

$ hdfs dfs -getfacl file

// display free space
$ hdfs dfs -df 

$ hdfs dfs -test -[defsz] fileordir

$ hadoop fs truncate file

// appends the content of a file test1 to a test2
$ hdfs dfs -appendToFile local.test1 hdfs.test2


// turn off the safemode
$ hdfs dfsadmin -safemode leave


$ hdfs dfsadmin -refreshNodes

$ hadoop archive -archive archiveName inputLocation outputLocation

$ hadoop distcp nodeOneUrl nodeTwoUrl

// disk balancer creates the plan , to destribute data evenly on the disks
$ hdfs diskbalancer -plan node1.mycluster.com

	plan command writes two output files , they are <nodename>.before.json which captures
the state of cluster before the diskbalancer is run.
	second file is <nodename>.plan.json


// execute the plan against the datanode
$ hdfs diskbalancer -execute /system/diskbalancer/nodename.plan.json

// GET the current status of the diskbalancer from a datanode
$ hdfs diskbalancer -query nodename.mycluster.com

//cancel the running plan
$ hdfs diskbalancer -cancel /system/diskbalancer/nodename.plan.json
$ hdfs diskbalancer -cancel planId -node nodename

planId can be read from query command





